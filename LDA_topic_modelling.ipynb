{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import gensim\n",
    "\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "import string\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 50\n",
    "NUM_PASSES = 100\n",
    "NUM_WORDS_PER_TOPIC = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\",\"VERB\"]):\n",
    "    # nlp = spacy.load(\"en_core_web_sm\", disable=[\"parset\", \"ner\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        new_text = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_postags:\n",
    "                new_text.append(token.lemma_)\n",
    "        new_text = [token for token in new_text if token not in stop_words]\n",
    "        final = \" \".join(new_text)\n",
    "        texts_out.append(final)\n",
    "\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        final.append(new)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_videos_path = \"./trending_videos/USvideos.csv\"\n",
    "in_df = pd.read_csv(in_videos_path)\n",
    "\n",
    "# Extracting transcripts and preprocessing \n",
    "documents = []\n",
    "documents_raw = []\n",
    "documents_vid = []\n",
    "i = 0\n",
    "for vid in in_df['video_id']:\n",
    "    print(vid)\n",
    "    try:\n",
    "        transcript_list = YouTubeTranscriptApi.get_transcript(vid)\n",
    "        transcript_text = ' '.join([item['text'] for item in transcript_list])\n",
    "        # preprocessed_text = preprocess_text(transcript_text)\n",
    "        # documents.append(preprocessed_text)\n",
    "        documents.append(transcript_text)\n",
    "        documents_raw.append(transcript_text)\n",
    "        documents_vid.append(vid)\n",
    "\n",
    "        i = i + 1\n",
    "        if i >= 150:\n",
    "            break\n",
    "    except:\n",
    "        print(\"Subtitles are turned off\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_documents = lemmatization(documents)\n",
    "documents = gen_words(lemmatized_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=NUM_TOPICS, passes=NUM_PASSES)\n",
    "\n",
    "print(\"Topics:\")\n",
    "for idx, topic in lda_model.print_topics(num_words=NUM_WORDS_PER_TOPIC, num_topics=NUM_TOPICS):\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "# Compute coherence score\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=documents, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Coherence Score: {coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [f\"word_{i}\" for i in range(1, NUM_WORDS_PER_TOPIC + 1)]\n",
    "\n",
    "# Initialize an empty list to collect data for DataFrame rows\n",
    "data_rows = []\n",
    "\n",
    "# Process each topic from the LDA model\n",
    "for idx, topic in lda_model.print_topics(num_words = NUM_WORDS_PER_TOPIC, num_topics=NUM_TOPICS):\n",
    "    # Split the topic string and extract words with scores\n",
    "    words_scores = [word.split(\"*\") for word in topic.split(\"+\")]\n",
    "    words_scores = [(word.strip().strip('\"'), float(score.strip())) for score, word in words_scores]\n",
    "    \n",
    "    # Sort words_scores by score (descending)\n",
    "    words_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Create a list of (word, score) tuples for the topic\n",
    "    word_score_tuples = [(word, score) for word, score in words_scores[:NUM_WORDS_PER_TOPIC]]\n",
    "    \n",
    "    # Append the row data (list of tuples) to the data_rows list\n",
    "    data_rows.append(word_score_tuples)\n",
    "\n",
    "# Create DataFrame from collected data_rows\n",
    "topics_df = pd.DataFrame(data_rows, columns=columns)\n",
    "\n",
    "# Add index for each topic (e.g., Topic 1, Topic 2, ...)\n",
    "topics_df.index = [f\"Topic {i}\" for i in range(len(topics_df))]\n",
    "\n",
    "# Display the topics DataFrame\n",
    "print(\"Topics DataFrame:\")\n",
    "print(topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Inference\n",
    "\n",
    "from gensim.models import TfidfModel\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "\n",
    "# Transform the corpus to TF-IDF space\n",
    "corpus_tfidf = tfidf_model[corpus]\n",
    "\n",
    "# # Print TF-IDF \n",
    "# for doc in corpus_tfidf:\n",
    "#     print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_distribution = [lda_model.get_document_topics(doc_tfidf) for doc_tfidf in corpus_tfidf]\n",
    "\n",
    "# Map documents to topics\n",
    "document_topics = []\n",
    "for doc_topics in doc_topic_distribution:\n",
    "    # Sort by probability \n",
    "    doc_topics.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    #taking top three topics\n",
    "    top_three_topics = doc_topics[:3]\n",
    "    document_topics.append(top_three_topics)\n",
    "\n",
    "\n",
    "for i, topics in enumerate(document_topics):\n",
    "    print(f\"Document {i} is assigned to topics:\")\n",
    "    for topic, prob in topics:\n",
    "        print(f\"  Topic {topic}: Probability {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(documents_vid[124])\n",
    "# print(\"\")\n",
    "# print(documents[124])\n",
    "# print(\"\")\n",
    "# print(documents_raw[124])\n",
    "# print(\"\")\n",
    "# print(lda_model.print_topics(num_words=NUM_WORDS_PER_TOPIC, num_topics=NUM_TOPICS)[39])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = max(topic_index for doc_topics in document_topics for topic_index, _ in doc_topics) + 1\n",
    "\n",
    "# Initialize topic_documents as a list of empty lists for each topic\n",
    "topic_documents = [[] for _ in range(num_topics)]\n",
    "\n",
    "# Populate topic_documents with document-number and its probability for each topic\n",
    "for doc_index, doc_topics in enumerate(document_topics):\n",
    "    for topic_index, prob in doc_topics:\n",
    "        # Append (document-number, probability) tuple to the corresponding topic sublist\n",
    "        topic_documents[topic_index].append((doc_index, prob))\n",
    "\n",
    "# Print the topic_documents array\n",
    "for topic_index, topic_docs in enumerate(topic_documents):\n",
    "    print(f\"Topic {topic_index}:\")\n",
    "    for doc_num, prob in topic_docs:\n",
    "        print(f\"  Document {doc_num}: Probability {prob:.3f}\")\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization_query(text, allowed_postags=[\"NOUN\", \"ADJ\",\"VERB\", \"ADV\"]):\n",
    "    # nlp = spacy.load(\"en_core_web_sm\", disable=[\"parset\", \"ner\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    texts_out = \"\"\n",
    "\n",
    "    doc = nlp(text)\n",
    "    new_text = []\n",
    "    for token in doc:\n",
    "        # if token.pos_ in allowed_postags:\n",
    "        #     new_text.append(token.lemma_)\n",
    "        new_text.append(token.lemma_)\n",
    "\n",
    "    # new_text = [token for token in new_text if token not in stop_words]\n",
    "    final = \" \".join(new_text)\n",
    "    texts_out = final\n",
    "\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_words_query(text):\n",
    "    final = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_numbers(topic_documents, user_topic_list):\n",
    "    # Sort user_topic_list by probability in descending order\n",
    "    user_topic_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Initialize a dictionary to collect document numbers and their relevance scores\n",
    "    relevance_scores = {}\n",
    "\n",
    "    # Iterate over user_topic_list to calculate relevance scores\n",
    "    for topic_index, topic_prob in user_topic_list:\n",
    "        # Get the list of document-number and probability tuples for the current topic\n",
    "        topic_docs = topic_documents[topic_index]\n",
    "        \n",
    "        # Update relevance_scores dictionary with document numbers and their relevance scores\n",
    "        for doc_num, doc_prob in topic_docs:\n",
    "            if doc_num not in relevance_scores:\n",
    "                relevance_scores[doc_num] = 0.0\n",
    "            # Accumulate relevance score based on topic probability and document probability\n",
    "            relevance_scores[doc_num] += topic_prob * doc_prob\n",
    "    \n",
    "    # Sort document numbers based on relevance scores in descending order\n",
    "    sorted_documents = sorted(relevance_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Extract sorted document numbers from the list of tuples\n",
    "    sorted_document_numbers = [doc_num for doc_num, _ in sorted_documents]\n",
    "    \n",
    "    return sorted_document_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_urls(document_numbers, document_vid):\n",
    "    base_url = \"https://www.youtube.com/watch?v=\"  # Base URL for YouTube videos\n",
    "    video_urls = []\n",
    "\n",
    "    # Iterate over the relevant document numbers\n",
    "    for doc_num in document_numbers:\n",
    "        if doc_num < len(document_vid):\n",
    "            video_id = document_vid[doc_num]\n",
    "            video_url = base_url + video_id\n",
    "            video_urls.append(video_url)\n",
    "        else:\n",
    "            print(f\"Document number {doc_num} is out of range of document_vid list.\")\n",
    "    \n",
    "    return video_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_videos():\n",
    "    user_query = input(\"Enter the search keywords\")\n",
    "    user_query_preprocessed = lemmatization_query(user_query)\n",
    "    print(user_query_preprocessed)\n",
    "    user_query_tokens = gen_words_query(user_query_preprocessed)\n",
    "    print(user_query_tokens)\n",
    "\n",
    "    user_query_bow = dictionary.doc2bow(user_query_tokens)\n",
    "\n",
    "    query_topic_distribution = lda_model.get_document_topics(user_query_bow)\n",
    "\n",
    "# Sort topics by probability and select top three\n",
    "    query_topics = sorted(query_topic_distribution, key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "# Print top topics for user prompt\n",
    "    print(f\"Top three topics for the prompt '{user_query}':\")\n",
    "    for topic, prob in query_topics:\n",
    "        print(f\"  Topic {topic}: Probability {prob:.3f}\")\n",
    "\n",
    "    print(query_topics)\n",
    "\n",
    "    document_numbers = get_document_numbers(topic_documents, query_topics)\n",
    "\n",
    "# Print the sorted document numbers based on relevance to the user's topics\n",
    "    print(\"Sorted Document Numbers based on Relevance:\")\n",
    "    print(document_numbers)\n",
    "\n",
    "    video_urls = get_video_urls(document_numbers, documents_vid)\n",
    "\n",
    "    print(\"Video URLs for Relevant Documents:\")\n",
    "    for url in video_urls:\n",
    "        print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    find_videos()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
