{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "807eddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import gensim\n",
    "\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "import string\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7374f092-ea72-4fd2-808a-f10393d96879",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 50\n",
    "NUM_PASSES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e8f1201-325f-41e4-96fc-a3e2074fd807",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82720c4a-f648-4280-ab26-7501481fe9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parset\", \"ner\"])\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        new_text = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_postags:\n",
    "                new_text.append(token.lemma_)\n",
    "        new_text = [token for token in new_text if token not in stop_words]\n",
    "        final = \" \".join(new_text)\n",
    "        texts_out.append(final)\n",
    "\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b085dd4e-84c2-41d9-9ea2-e353b362aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        final.append(new)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "096bef62-310a-4d87-a1ec-99d45f00b4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a756be7-ddd7-407b-b1a6-cbf9584afacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2kyS6SvSYSE\n",
      "Subtitles are turned off\n",
      "\n",
      "1ZAPwfrtAFY\n",
      "\n",
      "5qpjK5DgCt4\n",
      "Subtitles are turned off\n",
      "\n",
      "puqaWrEC7tY\n",
      "\n",
      "d380meD0W0M\n",
      "\n",
      "gHZ1Qz0KiKM\n",
      "Subtitles are turned off\n",
      "\n",
      "39idVpFF7NQ\n",
      "\n",
      "nc99ccSXST0\n",
      "\n",
      "jr9QtXwC9vc\n",
      "Subtitles are turned off\n",
      "\n",
      "TUmyygCMMGA\n",
      "\n",
      "9wRQljFNDW8\n",
      "\n",
      "VifQlJit6A0\n",
      "Subtitles are turned off\n",
      "\n",
      "5E4ZBSInqUU\n"
     ]
    }
   ],
   "source": [
    "in_videos_path = \"./trending_videos/USvideos.csv\"\n",
    "in_df = pd.read_csv(in_videos_path)\n",
    "\n",
    "# Extracting transcripts and preprocessing \n",
    "documents = []\n",
    "documents_directory = []\n",
    "i = 0\n",
    "for vid in in_df['video_id']:\n",
    "    print(vid)\n",
    "    try:\n",
    "        transcript_list = YouTubeTranscriptApi.get_transcript(vid)\n",
    "        transcript_text = ' '.join([item['text'] for item in transcript_list])\n",
    "        # preprocessed_text = preprocess_text(transcript_text)\n",
    "        # documents.append(preprocessed_text)\n",
    "        documents.append(transcript_text)\n",
    "\n",
    "        i = i + 1\n",
    "        if i > 100:\n",
    "            break\n",
    "    except:\n",
    "        print(\"Subtitles are turned off\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06970775-6aa3-483e-b6c1-8662e8aad8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_documents = lemmatization(documents)\n",
    "documents = gen_words(lemmatized_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abed2c7f-a210-40cf-9f37-48c285089d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=NUM_TOPICS, passes=NUM_PASSES)\n",
    "\n",
    "print(\"Topics:\")\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "# Compute coherence score\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=documents, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Coherence Score: {coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e77a2a-d592-414e-a8c6-231818f8a2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame(columns=[f\"Word_{i}\" for i in range(1, 11)])\n",
    "\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    words_scores = [word.split(\"*\") for word in topic.split(\"+\")]\n",
    "    words_scores = [(word.strip(), float(score.split(\"*\")[0])) for score, word in words_scores]\n",
    "    \n",
    "    words_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    row_data = {f\"Word_{i}\": word_score for i, word_score in enumerate(words_scores, start=1)}\n",
    "    topics_df = pd.concat([topics_df, pd.DataFrame(row_data)], ignore_index=True)\n",
    "\n",
    "topics_df.index = [f\"Topic {i}\" for i in range(1, len(topics_df) + 1)]\n",
    "\n",
    "print(\"Topics DataFrame:\")\n",
    "print(topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb991ad-1a17-406b-9652-ebf1e507ccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Inference\n",
    "\n",
    "from gensim.models import TfidfModel\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "\n",
    "# Transform the corpus to TF-IDF space\n",
    "corpus_tfidf = tfidf_model[corpus]\n",
    "\n",
    "# # Print TF-IDF \n",
    "# for doc in corpus_tfidf:\n",
    "#     print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec8322b-6e81-4260-ad49-61161cfc06e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_distribution = [lda_model.get_document_topics(doc_tfidf) for doc_tfidf in corpus_tfidf]\n",
    "\n",
    "# Map documents to topics\n",
    "document_topics = []\n",
    "for doc_topics in doc_topic_distribution:\n",
    "    # Sort by probability \n",
    "    doc_topics.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    #taking top three topics\n",
    "    top_three_topics = doc_topics[:3]\n",
    "    document_topics.append(top_three_topics)\n",
    "\n",
    "\n",
    "for i, topics in enumerate(document_topics):\n",
    "    print(f\"Document {i} is assigned to topics:\")\n",
    "    for topic, prob in topics:\n",
    "        print(f\"  Topic {topic}: Probability {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4180919a-af61-4cfd-9b01-c4423c83178e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
