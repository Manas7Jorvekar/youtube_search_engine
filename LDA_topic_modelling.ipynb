{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "807eddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import gensim\n",
    "\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "import string\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e8f1201-325f-41e4-96fc-a3e2074fd807",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82720c4a-f648-4280-ab26-7501481fe9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parset\", \"ner\"])\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        new_text = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_postags:\n",
    "                new_text.append(token.lemma_)\n",
    "        new_text = [token for token in new_text if token not in stop_words]\n",
    "        final = \" \".join(new_text)\n",
    "        texts_out.append(final)\n",
    "\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b085dd4e-84c2-41d9-9ea2-e353b362aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        final.append(new)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "096bef62-310a-4d87-a1ec-99d45f00b4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a756be7-ddd7-407b-b1a6-cbf9584afacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2kyS6SvSYSE\n",
      "\n",
      "1ZAPwfrtAFY\n",
      "\n",
      "5qpjK5DgCt4\n",
      "Subtitles are turned off\n",
      "\n",
      "puqaWrEC7tY\n",
      "\n",
      "d380meD0W0M\n",
      "\n",
      "gHZ1Qz0KiKM\n",
      "\n",
      "39idVpFF7NQ\n",
      "\n",
      "nc99ccSXST0\n",
      "\n",
      "jr9QtXwC9vc\n",
      "\n",
      "TUmyygCMMGA\n",
      "\n",
      "9wRQljFNDW8\n",
      "\n",
      "VifQlJit6A0\n",
      "Subtitles are turned off\n",
      "\n",
      "5E4ZBSInqUU\n"
     ]
    }
   ],
   "source": [
    "in_videos_path = \"./trending_videos/USvideos.csv\"\n",
    "in_df = pd.read_csv(in_videos_path)\n",
    "\n",
    "    # Extracting transcripts and preprocessing \n",
    "documents = []\n",
    "documents_directory = []\n",
    "i = 0\n",
    "for vid in in_df['video_id']:\n",
    "    print(vid)\n",
    "    try:\n",
    "        transcript_list = YouTubeTranscriptApi.get_transcript(vid)\n",
    "        transcript_text = ' '.join([item['text'] for item in transcript_list])\n",
    "        # preprocessed_text = preprocess_text(transcript_text)\n",
    "        # documents.append(preprocessed_text)\n",
    "        documents.append(transcript_text)\n",
    "\n",
    "        i = i + 1\n",
    "        if i > 10:\n",
    "            break\n",
    "    except:\n",
    "        print(\"Subtitles are turned off\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06970775-6aa3-483e-b6c1-8662e8aad8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_documents = lemmatization(documents)\n",
    "documents = gen_words(lemmatized_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abed2c7f-a210-40cf-9f37-48c285089d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "Topic 0: 0.029*\"go\" + 0.022*\"ice\" + 0.020*\"cream\" + 0.019*\"get\" + 0.017*\"make\" + 0.013*\"let\" + 0.010*\"want\" + 0.009*\"come\" + 0.009*\"see\" + 0.008*\"think\"\n",
      "Topic 1: 0.001*\"go\" + 0.001*\"make\" + 0.001*\"get\" + 0.001*\"know\" + 0.001*\"thing\" + 0.001*\"really\" + 0.001*\"want\" + 0.001*\"phone\" + 0.001*\"say\" + 0.001*\"think\"\n",
      "Topic 2: 0.024*\"drum\" + 0.015*\"say\" + 0.012*\"come\" + 0.012*\"kicking\" + 0.010*\"go\" + 0.010*\"hear\" + 0.009*\"get\" + 0.008*\"block\" + 0.008*\"know\" + 0.007*\"president\"\n",
      "Topic 3: 0.024*\"go\" + 0.022*\"get\" + 0.015*\"face\" + 0.013*\"really\" + 0.012*\"phone\" + 0.012*\"thing\" + 0.011*\"know\" + 0.011*\"right\" + 0.010*\"see\" + 0.010*\"point\"\n",
      "Topic 4: 0.016*\"job\" + 0.012*\"technology\" + 0.010*\"automation\" + 0.009*\"work\" + 0.009*\"see\" + 0.007*\"go\" + 0.007*\"people\" + 0.007*\"new\" + 0.006*\"really\" + 0.006*\"thing\"\n",
      "Coherence Score: 0.375266417023381\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, passes=10)\n",
    "\n",
    "print(\"Topics:\")\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "# Compute coherence score\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=documents, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Coherence Score: {coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e77a2a-d592-414e-a8c6-231818f8a2fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
