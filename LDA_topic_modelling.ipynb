{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "807eddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import gensim\n",
    "\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "import string\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7374f092-ea72-4fd2-808a-f10393d96879",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 20\n",
    "NUM_PASSES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e8f1201-325f-41e4-96fc-a3e2074fd807",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82720c4a-f648-4280-ab26-7501481fe9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parset\", \"ner\"])\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        new_text = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_postags:\n",
    "                new_text.append(token.lemma_)\n",
    "        new_text = [token for token in new_text if token not in stop_words]\n",
    "        final = \" \".join(new_text)\n",
    "        texts_out.append(final)\n",
    "\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b085dd4e-84c2-41d9-9ea2-e353b362aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        final.append(new)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "096bef62-310a-4d87-a1ec-99d45f00b4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a756be7-ddd7-407b-b1a6-cbf9584afacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2kyS6SvSYSE\n",
      "\n",
      "1ZAPwfrtAFY\n",
      "\n",
      "5qpjK5DgCt4\n",
      "Subtitles are turned off\n",
      "\n",
      "puqaWrEC7tY\n",
      "\n",
      "d380meD0W0M\n",
      "\n",
      "gHZ1Qz0KiKM\n",
      "\n",
      "39idVpFF7NQ\n",
      "\n",
      "nc99ccSXST0\n",
      "\n",
      "jr9QtXwC9vc\n",
      "\n",
      "TUmyygCMMGA\n",
      "\n",
      "9wRQljFNDW8\n",
      "\n",
      "VifQlJit6A0\n",
      "Subtitles are turned off\n",
      "\n",
      "5E4ZBSInqUU\n",
      "\n",
      "GgVmn66oK_A\n",
      "\n",
      "TaTleo4cOs8\n",
      "\n",
      "kgaO45SyaO4\n",
      "Subtitles are turned off\n",
      "\n",
      "ZAQs-ctOqXQ\n",
      "\n",
      "YVfyYrEmzgM\n",
      "\n",
      "eNSN6qet1kE\n",
      "\n",
      "B5HORANmzHw\n",
      "\n",
      "vU14JY3x81A\n",
      "\n",
      "6VhU_T463sU\n",
      "\n",
      "_-aDHxoblr4\n",
      "\n",
      "JBZTZZAcFTw\n",
      "\n",
      "lZ68j2J_GOM\n",
      "\n",
      "dRpNZV18N_g\n",
      "\n",
      "fcVjitaM3LY\n",
      "\n",
      "qeWvgZLz9yU\n",
      "\n",
      "iIxy3JN3-jc\n",
      "\n",
      "n30k5CwLhS4\n",
      "Subtitles are turned off\n",
      "\n",
      "U0hAC8O7RoI\n",
      "\n",
      "CBVGjS_EJok\n",
      "Subtitles are turned off\n",
      "\n",
      "n1WpP7iowLc\n",
      "Subtitles are turned off\n",
      "\n",
      "hz7ukDjuq4w\n",
      "Subtitles are turned off\n",
      "\n",
      "p2hJxyF7mok\n",
      "Subtitles are turned off\n",
      "\n",
      "0mlNzVSJrT0\n",
      "Subtitles are turned off\n",
      "\n",
      "Om_zGhJLZ5U\n",
      "\n",
      "e_7zHm7GsYc\n",
      "Subtitles are turned off\n",
      "\n",
      "dQvIbulWCM4\n",
      "Subtitles are turned off\n",
      "\n",
      "zZ9FciUx6gs\n",
      "Subtitles are turned off\n",
      "\n",
      "PaJCFHXcWmM\n",
      "Subtitles are turned off\n",
      "\n",
      "goP4Z5wyOlM\n",
      "\n",
      "NZFhMSgbKKM\n",
      "\n",
      "0tO_l_Ed5Rs\n",
      "Subtitles are turned off\n",
      "\n",
      "STI2fI7sKMo\n",
      "Subtitles are turned off\n",
      "\n",
      "BWPrk9PUwQE\n",
      "Subtitles are turned off\n",
      "\n",
      "ogYum4kWXgk\n",
      "Subtitles are turned off\n",
      "\n",
      "sbcbvuitiTc\n",
      "Subtitles are turned off\n",
      "\n",
      "zNqCVTs38nU\n",
      "\n",
      "DM-ni_LSOFE\n",
      "\n",
      "0PpNlNJ6Nng\n",
      "\n",
      "c-3JxzN2u34\n",
      "\n",
      "069D0NmW39o\n",
      "\n",
      "9t9u_yPEidY\n",
      "\n",
      "KODzih-pYlU\n",
      "\n",
      "8NHA23f7LvU\n",
      "\n",
      "w0XYVssCKjw\n",
      "\n",
      "9XSULcLI6Hg\n",
      "Subtitles are turned off\n",
      "\n",
      "pjuPpVLXDQg\n",
      "Subtitles are turned off\n",
      "\n",
      "9xSx-5tmq44\n",
      "\n",
      "4v0nOAzcG2A\n",
      "\n",
      "oTObwUiXdYY\n",
      "Subtitles are turned off\n",
      "\n",
      "1cgK-BIrXes\n",
      "\n",
      "ujyTQNNjjDU\n",
      "Subtitles are turned off\n",
      "\n",
      "PABlVzeldaM\n",
      "Subtitles are turned off\n",
      "\n",
      "6340tX9M_eM\n",
      "\n",
      "8mhTWqWlQzU\n",
      "\n",
      "t4YAyT4ihIQ\n",
      "\n",
      "MNiweoKXwfg\n",
      "\n",
      "Jw1Y-zhQURU\n",
      "Subtitles are turned off\n",
      "\n",
      "2Vv-BfVoq4g\n",
      "\n",
      "UFYCTJlayS4\n",
      "\n",
      "cccyOn99s4Y\n",
      "\n",
      "abGCYPCk294\n",
      "\n",
      "lY_0mkYDZDU\n",
      "Subtitles are turned off\n",
      "\n",
      "k8NxqyGcZA4\n",
      "\n",
      "IE-xepGLVt8\n",
      "\n",
      "ObIQ0s02UHg\n",
      "Subtitles are turned off\n",
      "\n",
      "cOc3tsFWoRs\n",
      "\n",
      "AJYb0Yom5UQ\n",
      "\n",
      "gLswXiRLdkA\n",
      "\n",
      "MyEqfBkzESU\n",
      "\n",
      "wJYE0sFOteE\n",
      "\n",
      "L_5_slz4ke4\n",
      "\n",
      "OKoL852K9uA\n",
      "\n",
      "pa_oUisZZy0\n",
      "\n",
      "fNtLIcyjsnI\n",
      "\n",
      "_Iz83-Cmt6A\n",
      "Subtitles are turned off\n",
      "\n",
      "YlvCVbfS9M0\n",
      "\n",
      "arJ5h2TY3I4\n",
      "Subtitles are turned off\n",
      "\n",
      "-2RVw2_QyxQ\n",
      "Subtitles are turned off\n",
      "\n",
      "gEHCXl4J9Qo\n",
      "\n",
      "-OK9i1_YYYc\n",
      "Subtitles are turned off\n",
      "\n",
      "b4vTZx_AtHk\n",
      "Subtitles are turned off\n",
      "\n",
      "kOnQocd799Y\n",
      "\n",
      "e4FApt6z55c\n",
      "\n",
      "2XK4omx9uMU\n",
      "Subtitles are turned off\n",
      "\n",
      "xfmipNU4Odc\n",
      "Subtitles are turned off\n",
      "\n",
      "jp9hK-jY6yY\n",
      "\n",
      "RkHuWjiR-LM\n",
      "Subtitles are turned off\n",
      "\n",
      "fCTKDn3Q8xQ\n",
      "\n",
      "DIU3xPdhCBI\n",
      "\n",
      "wnwF1FHybDQ\n",
      "\n",
      "9oFP-pOMNwE\n",
      "Subtitles are turned off\n",
      "\n",
      "pz95u3UVpaM\n",
      "Subtitles are turned off\n",
      "\n",
      "g5c1bk8weaQ\n",
      "Subtitles are turned off\n",
      "\n",
      "yuBqWcSKrCk\n",
      "\n",
      "x9mh-SwSKas\n",
      "\n",
      "uiA4B5Y63IQ\n",
      "\n",
      "viyRD5z6ilQ\n",
      "Subtitles are turned off\n",
      "\n",
      "QFfEtKvXMAs\n",
      "\n",
      "xg9ebVTL9yE\n",
      "\n",
      "fbHbTBP_u7U\n",
      "\n",
      "Ja_GMU7-sjs\n",
      "\n",
      "QOksZ8VogRw\n",
      "\n",
      "RYs08kX3Ih4\n",
      "Subtitles are turned off\n",
      "\n",
      "tOFkFZbPdnQ\n",
      "Subtitles are turned off\n",
      "\n",
      "dW4wpGg64pE\n",
      "\n",
      "2VP846QcA_4\n",
      "\n",
      "_dhneCO4YEE\n",
      "\n",
      "EYkEshCOhEU\n",
      "\n",
      "Rq9-mW6HgQE\n",
      "\n",
      "k29YnfttqEU\n",
      "\n",
      "Eg_kW5fw6qU\n",
      "\n",
      "J_QGZspO4gg\n",
      "Subtitles are turned off\n",
      "\n",
      "PkRBTF6nt4E\n",
      "\n",
      "ozkqm2ifMw8\n",
      "Subtitles are turned off\n",
      "\n",
      "htvR_dBs3eg\n",
      "\n",
      "qEEtzzi1EII\n",
      "Subtitles are turned off\n",
      "\n",
      "1Wk8ZRgXQnY\n",
      "\n",
      "8l_e6bx8UG8\n",
      "Subtitles are turned off\n",
      "\n",
      "lsfzA7sWlOM\n",
      "\n",
      "RQDphL2MVlM\n",
      "\n",
      "X7flefV8tec\n",
      "\n",
      "cYw-oyJ7AEY\n",
      "Subtitles are turned off\n",
      "\n",
      "6nJw-jPQYVI\n",
      "Subtitles are turned off\n",
      "\n",
      "5x1FAiIq_pQ\n",
      "Subtitles are turned off\n",
      "\n",
      "LMCuKltaY3M\n",
      "Subtitles are turned off\n",
      "\n",
      "6Detw08jRhs\n",
      "Subtitles are turned off\n",
      "\n",
      "vd4zwINEcLY\n",
      "\n",
      "7fm7mll2qvg\n",
      "Subtitles are turned off\n",
      "\n",
      "5gFpcEKayz4\n",
      "Subtitles are turned off\n",
      "\n",
      "eHIY3HNNqzM\n",
      "Subtitles are turned off\n",
      "\n",
      "q-WipZ9p0wk\n",
      "\n",
      "zy0b9e40tK8\n",
      "Subtitles are turned off\n",
      "\n",
      "rL4WkebTT_U\n",
      "\n",
      "Y6eKxjMA9ek\n",
      "\n",
      "Q6Usd3_fbq8\n",
      "\n",
      "44NYFvhXmW8\n",
      "\n",
      "9wg3v-01yKQ\n",
      "\n",
      "F-j_6IuaYfw\n",
      "\n",
      "_qSW96a2aKY\n",
      "Subtitles are turned off\n",
      "\n",
      "afgvlR9WmIQ\n",
      "\n",
      "_CJN_ryETIY\n",
      "Subtitles are turned off\n",
      "\n",
      "P4YJwy_T9pM\n",
      "\n",
      "YqH4eWR7jDQ\n",
      "\n",
      "r96KpNTcog4\n",
      "\n",
      "B4gXsobd_ao\n",
      "\n",
      "x8eLIJAM58M\n",
      "\n",
      "6V_aZsATDsM\n",
      "\n",
      "qg0GdM60syI\n",
      "\n",
      "8j_BYfVEyhA\n",
      "\n",
      "9ymjcSvEyhk\n",
      "Subtitles are turned off\n",
      "\n",
      "oB3SAI2oCqk\n",
      "Subtitles are turned off\n",
      "\n",
      "HNa_UWX51_s\n",
      "\n",
      "08nkwgZIE4I\n",
      "Subtitles are turned off\n",
      "\n",
      "5Smb3tZD1Qc\n",
      "Subtitles are turned off\n",
      "\n",
      "YHDbsyyjld0\n",
      "\n",
      "uyl6WtHEYOQ\n",
      "\n",
      "bhoFXkVy8JA\n",
      "Subtitles are turned off\n",
      "\n",
      "_w58R1OGQFA\n",
      "Subtitles are turned off\n",
      "\n",
      "BEweWXa5Twk\n",
      "\n",
      "egXr0PU-kCY\n",
      "\n",
      "fIQ2Ty2OL34\n",
      "Subtitles are turned off\n",
      "\n",
      "d_j512i2k6c\n",
      "\n",
      "4d07RXYLsJE\n",
      "Subtitles are turned off\n",
      "\n",
      "DOIw62TXYkg\n",
      "Subtitles are turned off\n",
      "\n",
      "0lbJHqAGpbA\n",
      "Subtitles are turned off\n",
      "\n",
      "JuP1Z8xpRb8\n",
      "Subtitles are turned off\n",
      "\n",
      "CR9zYgr84QY\n",
      "\n",
      "1640fZpYBSY\n",
      "Subtitles are turned off\n",
      "\n",
      "M2-IpsWQUWs\n",
      "Subtitles are turned off\n",
      "\n",
      "7m630xvAJtA\n",
      "\n",
      "510KQ_LXGww\n",
      "\n",
      "CO9dIyNM_9Q\n",
      "\n",
      "hX643KbiI4s\n",
      "Subtitles are turned off\n",
      "\n",
      "mwpHSMv1pI4\n",
      "\n",
      "Mfiim71QdYQ\n",
      "Subtitles are turned off\n",
      "\n",
      "UFPSIa1cLRQ\n",
      "Subtitles are turned off\n",
      "\n",
      "o78x918zbFk\n",
      "\n",
      "iNf6VErGDLI\n",
      "Subtitles are turned off\n",
      "\n",
      "6ixU_vdE0Es\n",
      "\n",
      "wRGldR_SQAA\n",
      "Subtitles are turned off\n",
      "\n",
      "agKDPntMv-E\n",
      "\n",
      "Xl38SuxyrgM\n",
      "\n",
      "u7pV4vxD1bs\n",
      "\n",
      "zYWt2mnalP8\n",
      "Subtitles are turned off\n",
      "\n",
      "PoNWU2cox9w\n",
      "\n",
      "6hTzM1BPdU8\n",
      "\n",
      "MCW5HUkrr-o\n",
      "\n",
      "YvfYK0EEhK4\n",
      "\n",
      "cxMvzK2OQTw\n",
      "\n",
      "bAkEd8r7Nnw\n",
      "\n",
      "ItYOdWRo0JY\n",
      "\n",
      "5530I_pYjbo\n",
      "\n",
      "dUMH6DVYskc\n",
      "\n",
      "gjXrm2Q-te4\n",
      "\n",
      "9SK1I0V6U5c\n",
      "\n",
      "VsYmwBOYfW8\n",
      "\n",
      "kgaO45SyaO4\n",
      "Subtitles are turned off\n",
      "\n",
      "CtBca6H6Teg\n",
      "\n",
      "pcWKpGzhgq4\n",
      "\n",
      "L3br0klRqF4\n",
      "Subtitles are turned off\n",
      "\n",
      "9kF9xY74h-E\n",
      "Subtitles are turned off\n",
      "\n",
      "p2EY93WfBQk\n",
      "\n",
      "xL_qpDkF5A8\n",
      "\n",
      "eQVhAN7-IAw\n",
      "Subtitles are turned off\n",
      "\n",
      "2kyS6SvSYSE\n",
      "\n",
      "Y9nDagqKL7Q\n",
      "\n",
      "DbJ2s_g1oDc\n",
      "\n",
      "KJ12FVmumeA\n",
      "\n",
      "1ZAPwfrtAFY\n",
      "\n",
      "qDAxDcjgn-8\n",
      "Subtitles are turned off\n",
      "\n",
      "gHZ1Qz0KiKM\n",
      "\n",
      "815lSYphlbw\n",
      "\n",
      "jr9QtXwC9vc\n",
      "\n",
      "E_86MEdNsAI\n",
      "\n",
      "Cr9oGxr0Qa0\n",
      "\n",
      "g0K5QoEj7FQ\n",
      "\n",
      "l4bAoNAx2uo\n"
     ]
    }
   ],
   "source": [
    "in_videos_path = \"./trending_videos/USvideos.csv\"\n",
    "in_df = pd.read_csv(in_videos_path)\n",
    "\n",
    "# Extracting transcripts and preprocessing \n",
    "documents = []\n",
    "documents_raw = []\n",
    "documents_vid = []\n",
    "i = 0\n",
    "for vid in in_df['video_id']:\n",
    "    print(vid)\n",
    "    try:\n",
    "        transcript_list = YouTubeTranscriptApi.get_transcript(vid)\n",
    "        transcript_text = ' '.join([item['text'] for item in transcript_list])\n",
    "        # preprocessed_text = preprocess_text(transcript_text)\n",
    "        # documents.append(preprocessed_text)\n",
    "        documents.append(transcript_text)\n",
    "        documents_raw.append(transcript_text)\n",
    "        documents_vid.append(vid)\n",
    "\n",
    "        i = i + 1\n",
    "        if i > 150:\n",
    "            break\n",
    "    except:\n",
    "        print(\"Subtitles are turned off\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06970775-6aa3-483e-b6c1-8662e8aad8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_documents = lemmatization(documents)\n",
    "documents = gen_words(lemmatized_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abed2c7f-a210-40cf-9f37-48c285089d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "Topic 0: 0.024*\"get\" + 0.020*\"go\" + 0.018*\"let\" + 0.017*\"prime\" + 0.017*\"know\" + 0.011*\"really\" + 0.009*\"never\" + 0.009*\"thing\" + 0.008*\"time\" + 0.008*\"take\" + 0.008*\"see\" + 0.008*\"want\" + 0.007*\"start\" + 0.007*\"apartment\" + 0.007*\"love\"\n",
      "Topic 1: 0.021*\"go\" + 0.015*\"think\" + 0.015*\"movie\" + 0.013*\"see\" + 0.012*\"come\" + 0.012*\"know\" + 0.011*\"get\" + 0.011*\"guy\" + 0.010*\"time\" + 0.010*\"look\" + 0.009*\"good\" + 0.009*\"make\" + 0.008*\"say\" + 0.008*\"thing\" + 0.007*\"let\"\n",
      "Topic 2: 0.022*\"phone\" + 0.021*\"face\" + 0.021*\"really\" + 0.017*\"go\" + 0.016*\"see\" + 0.015*\"get\" + 0.014*\"video\" + 0.014*\"know\" + 0.014*\"thing\" + 0.011*\"take\" + 0.010*\"use\" + 0.009*\"look\" + 0.009*\"try\" + 0.009*\"cat\" + 0.009*\"lot\"\n",
      "Topic 3: 0.016*\"people\" + 0.010*\"earthquake\" + 0.008*\"happen\" + 0.007*\"much\" + 0.007*\"come\" + 0.006*\"day\" + 0.006*\"call\" + 0.006*\"know\" + 0.006*\"big\" + 0.005*\"thing\" + 0.005*\"say\" + 0.005*\"bad\" + 0.005*\"building\" + 0.005*\"see\" + 0.005*\"go\"\n",
      "Topic 4: 0.024*\"noise\" + 0.014*\"go\" + 0.012*\"speaker\" + 0.011*\"cancel\" + 0.008*\"player\" + 0.008*\"surface\" + 0.008*\"break\" + 0.008*\"get\" + 0.008*\"let\" + 0.008*\"think\" + 0.007*\"even\" + 0.007*\"sound\" + 0.007*\"video\" + 0.007*\"see\" + 0.006*\"vibration\"\n",
      "Topic 5: 0.019*\"college\" + 0.015*\"beam\" + 0.007*\"oak\" + 0.005*\"year\" + 0.005*\"new\" + 0.005*\"story\" + 0.005*\"ask\" + 0.005*\"become\" + 0.005*\"discover\" + 0.005*\"century\" + 0.005*\"beatly\" + 0.005*\"found\" + 0.003*\"go\" + 0.003*\"say\" + 0.003*\"way\"\n",
      "Topic 6: 0.033*\"dream\" + 0.021*\"app\" + 0.019*\"lucid\" + 0.007*\"find\" + 0.007*\"dreamer\" + 0.006*\"able\" + 0.006*\"picture\" + 0.006*\"also\" + 0.006*\"say\" + 0.006*\"brain\" + 0.006*\"use\" + 0.005*\"read\" + 0.005*\"board\" + 0.005*\"thing\" + 0.005*\"call\"\n",
      "Topic 7: 0.017*\"go\" + 0.014*\"get\" + 0.010*\"say\" + 0.010*\"thing\" + 0.008*\"make\" + 0.007*\"look\" + 0.007*\"know\" + 0.007*\"really\" + 0.007*\"actually\" + 0.006*\"see\" + 0.006*\"think\" + 0.006*\"use\" + 0.005*\"right\" + 0.005*\"want\" + 0.005*\"take\"\n",
      "Topic 8: 0.024*\"crush\" + 0.018*\"amor\" + 0.015*\"country\" + 0.008*\"vida\" + 0.007*\"war\" + 0.007*\"government\" + 0.006*\"asi\" + 0.006*\"que\" + 0.006*\"world\" + 0.006*\"year\" + 0.005*\"fail\" + 0.005*\"lead\" + 0.004*\"relationship\" + 0.004*\"shut\" + 0.004*\"cuerpo\"\n",
      "Topic 9: 0.025*\"go\" + 0.017*\"one\" + 0.016*\"feel\" + 0.012*\"outfit\" + 0.011*\"think\" + 0.011*\"josh\" + 0.011*\"want\" + 0.009*\"squat\" + 0.009*\"pant\" + 0.009*\"way\" + 0.008*\"right\" + 0.008*\"kind\" + 0.008*\"toilet\" + 0.008*\"little\" + 0.008*\"suspender\"\n",
      "Topic 10: 0.045*\"none\" + 0.029*\"baby\" + 0.029*\"hey\" + 0.022*\"business\" + 0.013*\"think\" + 0.009*\"crazy\" + 0.009*\"la\" + 0.009*\"losin\" + 0.007*\"pay\" + 0.005*\"go\" + 0.005*\"say\" + 0.005*\"pretty\" + 0.005*\"always\" + 0.005*\"hard\" + 0.005*\"kinda\"\n",
      "Topic 11: 0.014*\"movie\" + 0.012*\"go\" + 0.012*\"know\" + 0.012*\"see\" + 0.011*\"get\" + 0.009*\"make\" + 0.008*\"time\" + 0.007*\"take\" + 0.007*\"really\" + 0.007*\"bike\" + 0.007*\"year\" + 0.006*\"say\" + 0.006*\"actually\" + 0.006*\"love\" + 0.006*\"believe\"\n",
      "Topic 12: 0.046*\"know\" + 0.027*\"go\" + 0.024*\"get\" + 0.016*\"say\" + 0.014*\"think\" + 0.013*\"make\" + 0.012*\"want\" + 0.011*\"really\" + 0.011*\"little\" + 0.010*\"good\" + 0.010*\"right\" + 0.010*\"kind\" + 0.009*\"thing\" + 0.009*\"mean\" + 0.009*\"guy\"\n",
      "Topic 13: 0.021*\"job\" + 0.020*\"technology\" + 0.014*\"automation\" + 0.010*\"point\" + 0.009*\"problem\" + 0.008*\"battery\" + 0.008*\"production\" + 0.008*\"robot\" + 0.008*\"worker\" + 0.008*\"new\" + 0.007*\"thank\" + 0.007*\"exclamation\" + 0.007*\"labor\" + 0.006*\"accord\" + 0.006*\"computer\"\n",
      "Topic 14: 0.015*\"know\" + 0.015*\"go\" + 0.012*\"see\" + 0.011*\"thing\" + 0.010*\"seem\" + 0.010*\"cheese\" + 0.009*\"people\" + 0.008*\"get\" + 0.008*\"say\" + 0.007*\"time\" + 0.007*\"good\" + 0.006*\"come\" + 0.006*\"look\" + 0.006*\"tell\" + 0.006*\"make\"\n",
      "Topic 15: 0.037*\"go\" + 0.027*\"get\" + 0.013*\"know\" + 0.012*\"right\" + 0.011*\"good\" + 0.010*\"see\" + 0.010*\"let\" + 0.010*\"think\" + 0.009*\"look\" + 0.008*\"put\" + 0.007*\"make\" + 0.007*\"drum\" + 0.007*\"come\" + 0.007*\"want\" + 0.007*\"say\"\n",
      "Topic 16: 0.000*\"go\" + 0.000*\"look\" + 0.000*\"really\" + 0.000*\"get\" + 0.000*\"know\" + 0.000*\"see\" + 0.000*\"use\" + 0.000*\"little\" + 0.000*\"right\" + 0.000*\"think\" + 0.000*\"good\" + 0.000*\"thing\" + 0.000*\"make\" + 0.000*\"guy\" + 0.000*\"lot\"\n",
      "Topic 17: 0.073*\"go\" + 0.029*\"look\" + 0.028*\"use\" + 0.023*\"really\" + 0.017*\"face\" + 0.017*\"little\" + 0.015*\"brush\" + 0.015*\"eye\" + 0.014*\"bit\" + 0.014*\"shade\" + 0.014*\"color\" + 0.013*\"foundation\" + 0.013*\"apply\" + 0.011*\"makeup\" + 0.011*\"nose\"\n",
      "Topic 18: 0.020*\"go\" + 0.015*\"movie\" + 0.015*\"lie\" + 0.014*\"night\" + 0.014*\"see\" + 0.014*\"detector\" + 0.012*\"love\" + 0.011*\"buzz\" + 0.011*\"know\" + 0.010*\"last\" + 0.010*\"baby\" + 0.009*\"drive\" + 0.009*\"think\" + 0.008*\"dad\" + 0.007*\"ooh\"\n",
      "Topic 19: 0.037*\"go\" + 0.026*\"get\" + 0.017*\"think\" + 0.014*\"know\" + 0.013*\"look\" + 0.013*\"see\" + 0.013*\"guy\" + 0.012*\"right\" + 0.012*\"make\" + 0.010*\"good\" + 0.010*\"let\" + 0.009*\"really\" + 0.009*\"want\" + 0.008*\"say\" + 0.008*\"video\"\n",
      "Coherence Score: 0.309437450146097\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=NUM_TOPICS, passes=NUM_PASSES)\n",
    "\n",
    "print(\"Topics:\")\n",
    "for idx, topic in lda_model.print_topics(num_words=15):\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "# Compute coherence score\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=documents, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Coherence Score: {coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e34b3617-4d43-4971-8ce8-7d5a9b3d1e4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "15 columns passed, passed data had 10 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/COEP/sem6/DSci/project/venv/lib/python3.11/site-packages/pandas/core/internals/construction.py:939\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_or_indexify_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[0;32m~/COEP/sem6/DSci/project/venv/lib/python3.11/site-packages/pandas/core/internals/construction.py:986\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[0;34m(content, columns)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    989\u001b[0m     )\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_mi_list:\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 15 columns passed, passed data had 10 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     data_rows\u001b[38;5;241m.\u001b[39mappend(word_score_tuples)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Create DataFrame from collected data_rows\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m topics_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Add index for each topic (e.g., Topic 1, Topic 2, ...)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m topics_df\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopic \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(topics_df))]\n",
      "File \u001b[0;32m~/COEP/sem6/DSci/project/venv/lib/python3.11/site-packages/pandas/core/frame.py:840\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    839\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 840\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    848\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    849\u001b[0m         arrays,\n\u001b[1;32m    850\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    853\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    854\u001b[0m     )\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/COEP/sem6/DSci/project/venv/lib/python3.11/site-packages/pandas/core/internals/construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/COEP/sem6/DSci/project/venv/lib/python3.11/site-packages/pandas/core/internals/construction.py:845\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    842\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    843\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m--> 845\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_finalize_columns_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[0;32m~/COEP/sem6/DSci/project/venv/lib/python3.11/site-packages/pandas/core/internals/construction.py:942\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[0;32m--> 942\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[1;32m    945\u001b[0m     contents \u001b[38;5;241m=\u001b[39m convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: 15 columns passed, passed data had 10 columns"
     ]
    }
   ],
   "source": [
    "num_words_per_topic = 15  # Number of words per topic\n",
    "columns = [f\"word_{i}\" for i in range(1, num_words_per_topic + 1)]\n",
    "\n",
    "# Initialize an empty list to collect data for DataFrame rows\n",
    "data_rows = []\n",
    "\n",
    "# Process each topic from the LDA model\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    # Split the topic string and extract words with scores\n",
    "    words_scores = [word.split(\"*\") for word in topic.split(\"+\")]\n",
    "    words_scores = [(word.strip().strip('\"'), float(score.strip())) for score, word in words_scores]\n",
    "    \n",
    "    # Sort words_scores by score (descending)\n",
    "    words_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Create a list of (word, score) tuples for the topic\n",
    "    word_score_tuples = [(word, score) for word, score in words_scores[:num_words_per_topic]]\n",
    "    \n",
    "    # Append the row data (list of tuples) to the data_rows list\n",
    "    data_rows.append(word_score_tuples)\n",
    "\n",
    "# Create DataFrame from collected data_rows\n",
    "topics_df = pd.DataFrame(data_rows, columns=columns)\n",
    "\n",
    "# Add index for each topic (e.g., Topic 1, Topic 2, ...)\n",
    "topics_df.index = [f\"Topic {i + 1}\" for i in range(len(topics_df))]\n",
    "\n",
    "# Display the topics DataFrame\n",
    "print(\"Topics DataFrame:\")\n",
    "print(topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbb991ad-1a17-406b-9652-ebf1e507ccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Inference\n",
    "\n",
    "from gensim.models import TfidfModel\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "\n",
    "# Transform the corpus to TF-IDF space\n",
    "corpus_tfidf = tfidf_model[corpus]\n",
    "\n",
    "# # Print TF-IDF \n",
    "# for doc in corpus_tfidf:\n",
    "#     print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cec8322b-6e81-4260-ad49-61161cfc06e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 is assigned to topics:\n",
      "  Topic 19: Probability 0.929\n",
      "Document 1 is assigned to topics:\n",
      "  Topic 7: Probability 0.959\n",
      "Document 2 is assigned to topics:\n",
      "  Topic 19: Probability 0.834\n",
      "  Topic 12: Probability 0.089\n",
      "Document 3 is assigned to topics:\n",
      "  Topic 15: Probability 0.922\n",
      "Document 4 is assigned to topics:\n",
      "  Topic 2: Probability 0.936\n",
      "Document 5 is assigned to topics:\n",
      "  Topic 15: Probability 0.590\n",
      "  Topic 14: Probability 0.164\n",
      "  Topic 3: Probability 0.143\n",
      "Document 6 is assigned to topics:\n",
      "  Topic 15: Probability 0.875\n",
      "Document 7 is assigned to topics:\n",
      "  Topic 14: Probability 0.887\n",
      "Document 8 is assigned to topics:\n",
      "  Topic 13: Probability 0.380\n",
      "  Topic 11: Probability 0.364\n",
      "  Topic 19: Probability 0.135\n",
      "Document 9 is assigned to topics:\n",
      "  Topic 6: Probability 0.848\n",
      "Document 10 is assigned to topics:\n",
      "  Topic 15: Probability 0.648\n",
      "  Topic 0: Probability 0.019\n",
      "  Topic 1: Probability 0.019\n",
      "Document 11 is assigned to topics:\n",
      "  Topic 8: Probability 0.926\n",
      "Document 12 is assigned to topics:\n",
      "  Topic 12: Probability 0.926\n",
      "Document 13 is assigned to topics:\n",
      "  Topic 7: Probability 0.689\n",
      "  Topic 18: Probability 0.221\n",
      "Document 14 is assigned to topics:\n",
      "  Topic 1: Probability 0.827\n",
      "  Topic 4: Probability 0.087\n",
      "Document 15 is assigned to topics:\n",
      "  Topic 7: Probability 0.915\n",
      "Document 16 is assigned to topics:\n",
      "  Topic 3: Probability 0.927\n",
      "Document 17 is assigned to topics:\n",
      "  Topic 6: Probability 0.855\n",
      "Document 18 is assigned to topics:\n",
      "  Topic 3: Probability 0.689\n",
      "  Topic 19: Probability 0.235\n",
      "Document 19 is assigned to topics:\n",
      "  Topic 18: Probability 0.912\n",
      "Document 20 is assigned to topics:\n",
      "  Topic 0: Probability 0.906\n",
      "Document 21 is assigned to topics:\n",
      "  Topic 9: Probability 0.818\n",
      "Document 22 is assigned to topics:\n",
      "  Topic 15: Probability 0.880\n",
      "Document 23 is assigned to topics:\n",
      "  Topic 0: Probability 0.872\n",
      "Document 24 is assigned to topics:\n",
      "  Topic 1: Probability 0.782\n",
      "  Topic 7: Probability 0.129\n",
      "  Topic 9: Probability 0.016\n",
      "Document 25 is assigned to topics:\n",
      "  Topic 17: Probability 0.464\n",
      "  Topic 12: Probability 0.444\n",
      "Document 26 is assigned to topics:\n",
      "  Topic 19: Probability 0.913\n",
      "Document 27 is assigned to topics:\n",
      "  Topic 7: Probability 0.731\n",
      "  Topic 3: Probability 0.174\n",
      "  Topic 11: Probability 0.049\n",
      "Document 28 is assigned to topics:\n",
      "  Topic 3: Probability 0.424\n",
      "  Topic 14: Probability 0.418\n",
      "Document 29 is assigned to topics:\n",
      "  Topic 15: Probability 0.432\n",
      "  Topic 6: Probability 0.280\n",
      "  Topic 0: Probability 0.081\n",
      "Document 30 is assigned to topics:\n",
      "  Topic 7: Probability 0.819\n",
      "Document 31 is assigned to topics:\n",
      "  Topic 1: Probability 0.740\n",
      "  Topic 2: Probability 0.159\n",
      "Document 32 is assigned to topics:\n",
      "  Topic 19: Probability 0.935\n",
      "Document 33 is assigned to topics:\n",
      "  Topic 3: Probability 0.921\n",
      "Document 34 is assigned to topics:\n",
      "  Topic 19: Probability 0.876\n",
      "Document 35 is assigned to topics:\n",
      "  Topic 8: Probability 0.835\n",
      "Document 36 is assigned to topics:\n",
      "  Topic 19: Probability 0.913\n",
      "Document 37 is assigned to topics:\n",
      "  Topic 19: Probability 0.462\n",
      "  Topic 11: Probability 0.393\n",
      "Document 38 is assigned to topics:\n",
      "  Topic 0: Probability 0.917\n",
      "Document 39 is assigned to topics:\n",
      "  Topic 18: Probability 0.898\n",
      "Document 40 is assigned to topics:\n",
      "  Topic 7: Probability 0.717\n",
      "  Topic 15: Probability 0.197\n",
      "Document 41 is assigned to topics:\n",
      "  Topic 7: Probability 0.797\n",
      "  Topic 8: Probability 0.011\n",
      "  Topic 0: Probability 0.011\n",
      "Document 42 is assigned to topics:\n",
      "  Topic 8: Probability 0.813\n",
      "  Topic 1: Probability 0.043\n",
      "Document 43 is assigned to topics:\n",
      "  Topic 19: Probability 0.636\n",
      "  Topic 17: Probability 0.305\n",
      "Document 44 is assigned to topics:\n",
      "  Topic 19: Probability 0.895\n",
      "Document 45 is assigned to topics:\n",
      "  Topic 14: Probability 0.727\n",
      "  Topic 2: Probability 0.144\n",
      "Document 46 is assigned to topics:\n",
      "  Topic 0: Probability 0.843\n",
      "Document 47 is assigned to topics:\n",
      "  Topic 6: Probability 0.701\n",
      "  Topic 19: Probability 0.228\n",
      "Document 48 is assigned to topics:\n",
      "  Topic 7: Probability 0.860\n",
      "  Topic 19: Probability 0.068\n",
      "Document 49 is assigned to topics:\n",
      "  Topic 1: Probability 0.937\n",
      "  Topic 19: Probability 0.016\n",
      "Document 50 is assigned to topics:\n",
      "  Topic 14: Probability 0.910\n",
      "Document 51 is assigned to topics:\n",
      "  Topic 3: Probability 0.858\n",
      "  Topic 19: Probability 0.064\n",
      "Document 52 is assigned to topics:\n",
      "  Topic 18: Probability 0.857\n",
      "Document 53 is assigned to topics:\n",
      "  Topic 17: Probability 0.649\n",
      "  Topic 0: Probability 0.154\n",
      "  Topic 12: Probability 0.084\n",
      "Document 54 is assigned to topics:\n",
      "  Topic 7: Probability 0.533\n",
      "  Topic 19: Probability 0.359\n",
      "  Topic 12: Probability 0.052\n",
      "Document 55 is assigned to topics:\n",
      "  Topic 17: Probability 0.933\n",
      "Document 56 is assigned to topics:\n",
      "  Topic 14: Probability 0.885\n",
      "Document 57 is assigned to topics:\n",
      "  Topic 4: Probability 0.900\n",
      "Document 58 is assigned to topics:\n",
      "  Topic 19: Probability 0.612\n",
      "  Topic 17: Probability 0.288\n",
      "Document 59 is assigned to topics:\n",
      "  Topic 13: Probability 0.791\n",
      "  Topic 0: Probability 0.011\n",
      "  Topic 1: Probability 0.011\n",
      "Document 60 is assigned to topics:\n",
      "  Topic 9: Probability 0.620\n",
      "  Topic 19: Probability 0.178\n",
      "  Topic 18: Probability 0.100\n",
      "Document 61 is assigned to topics:\n",
      "  Topic 13: Probability 0.766\n",
      "  Topic 19: Probability 0.117\n",
      "Document 62 is assigned to topics:\n",
      "  Topic 8: Probability 0.842\n",
      "Document 63 is assigned to topics:\n",
      "  Topic 15: Probability 0.803\n",
      "  Topic 19: Probability 0.115\n",
      "  Topic 18: Probability 0.040\n",
      "Document 64 is assigned to topics:\n",
      "  Topic 18: Probability 0.622\n",
      "  Topic 10: Probability 0.108\n",
      "  Topic 11: Probability 0.049\n",
      "Document 65 is assigned to topics:\n",
      "  Topic 8: Probability 0.638\n",
      "  Topic 19: Probability 0.256\n",
      "  Topic 12: Probability 0.021\n",
      "Document 66 is assigned to topics:\n",
      "  Topic 9: Probability 0.834\n",
      "Document 67 is assigned to topics:\n",
      "  Topic 7: Probability 0.593\n",
      "  Topic 19: Probability 0.312\n",
      "Document 68 is assigned to topics:\n",
      "  Topic 17: Probability 0.594\n",
      "  Topic 12: Probability 0.317\n",
      "Document 69 is assigned to topics:\n",
      "  Topic 11: Probability 0.920\n",
      "Document 70 is assigned to topics:\n",
      "  Topic 19: Probability 0.932\n",
      "Document 71 is assigned to topics:\n",
      "  Topic 18: Probability 0.872\n",
      "Document 72 is assigned to topics:\n",
      "  Topic 4: Probability 0.805\n",
      "  Topic 0: Probability 0.010\n",
      "  Topic 1: Probability 0.010\n",
      "Document 73 is assigned to topics:\n",
      "  Topic 0: Probability 0.794\n",
      "  Topic 1: Probability 0.011\n",
      "  Topic 2: Probability 0.011\n",
      "Document 74 is assigned to topics:\n",
      "  Topic 0: Probability 0.867\n",
      "Document 75 is assigned to topics:\n",
      "  Topic 7: Probability 0.919\n",
      "Document 76 is assigned to topics:\n",
      "  Topic 12: Probability 0.471\n",
      "  Topic 15: Probability 0.224\n",
      "  Topic 18: Probability 0.179\n",
      "Document 77 is assigned to topics:\n",
      "  Topic 1: Probability 0.813\n",
      "  Topic 12: Probability 0.110\n",
      "Document 78 is assigned to topics:\n",
      "  Topic 15: Probability 0.941\n",
      "Document 79 is assigned to topics:\n",
      "  Topic 3: Probability 0.927\n",
      "Document 80 is assigned to topics:\n",
      "  Topic 19: Probability 0.639\n",
      "  Topic 17: Probability 0.304\n",
      "Document 81 is assigned to topics:\n",
      "  Topic 14: Probability 0.804\n",
      "  Topic 18: Probability 0.109\n",
      "Document 82 is assigned to topics:\n",
      "  Topic 3: Probability 0.648\n",
      "  Topic 19: Probability 0.204\n",
      "  Topic 15: Probability 0.039\n",
      "Document 83 is assigned to topics:\n",
      "  Topic 11: Probability 0.872\n",
      "  Topic 3: Probability 0.062\n",
      "Document 84 is assigned to topics:\n",
      "  Topic 13: Probability 0.877\n",
      "Document 85 is assigned to topics:\n",
      "  Topic 7: Probability 0.813\n",
      "  Topic 19: Probability 0.116\n",
      "Document 86 is assigned to topics:\n",
      "  Topic 9: Probability 0.861\n",
      "Document 87 is assigned to topics:\n",
      "  Topic 0: Probability 0.830\n",
      "Document 88 is assigned to topics:\n",
      "  Topic 19: Probability 0.930\n",
      "Document 89 is assigned to topics:\n",
      "  Topic 12: Probability 0.697\n",
      "  Topic 3: Probability 0.225\n",
      "Document 90 is assigned to topics:\n",
      "  Topic 12: Probability 0.547\n",
      "  Topic 2: Probability 0.277\n",
      "Document 91 is assigned to topics:\n",
      "  Topic 12: Probability 0.935\n",
      "Document 92 is assigned to topics:\n",
      "  Topic 15: Probability 0.918\n",
      "Document 93 is assigned to topics:\n",
      "  Topic 19: Probability 0.930\n",
      "Document 94 is assigned to topics:\n",
      "  Topic 0: Probability 0.050\n",
      "  Topic 1: Probability 0.050\n",
      "  Topic 2: Probability 0.050\n",
      "Document 95 is assigned to topics:\n",
      "  Topic 11: Probability 0.858\n",
      "Document 96 is assigned to topics:\n",
      "  Topic 10: Probability 0.782\n",
      "  Topic 0: Probability 0.011\n",
      "  Topic 1: Probability 0.011\n",
      "Document 97 is assigned to topics:\n",
      "  Topic 12: Probability 0.895\n",
      "  Topic 7: Probability 0.030\n",
      "Document 98 is assigned to topics:\n",
      "  Topic 18: Probability 0.770\n",
      "  Topic 0: Probability 0.012\n",
      "  Topic 1: Probability 0.012\n",
      "Document 99 is assigned to topics:\n",
      "  Topic 9: Probability 0.769\n",
      "  Topic 19: Probability 0.166\n",
      "Document 100 is assigned to topics:\n",
      "  Topic 5: Probability 0.835\n",
      "Document 101 is assigned to topics:\n",
      "  Topic 15: Probability 0.917\n",
      "Document 102 is assigned to topics:\n",
      "  Topic 12: Probability 0.903\n",
      "Document 103 is assigned to topics:\n",
      "  Topic 11: Probability 0.583\n",
      "  Topic 12: Probability 0.297\n",
      "Document 104 is assigned to topics:\n",
      "  Topic 18: Probability 0.454\n",
      "  Topic 19: Probability 0.305\n",
      "  Topic 12: Probability 0.164\n",
      "Document 105 is assigned to topics:\n",
      "  Topic 17: Probability 0.548\n",
      "  Topic 11: Probability 0.243\n",
      "  Topic 0: Probability 0.012\n",
      "Document 106 is assigned to topics:\n",
      "  Topic 12: Probability 0.942\n",
      "Document 107 is assigned to topics:\n",
      "  Topic 18: Probability 0.870\n",
      "Document 108 is assigned to topics:\n",
      "  Topic 2: Probability 0.862\n",
      "Document 109 is assigned to topics:\n",
      "  Topic 18: Probability 0.839\n",
      "Document 110 is assigned to topics:\n",
      "  Topic 17: Probability 0.437\n",
      "  Topic 7: Probability 0.341\n",
      "  Topic 19: Probability 0.157\n",
      "Document 111 is assigned to topics:\n",
      "  Topic 1: Probability 0.575\n",
      "  Topic 17: Probability 0.357\n",
      "Document 112 is assigned to topics:\n",
      "  Topic 3: Probability 0.907\n",
      "Document 113 is assigned to topics:\n",
      "  Topic 7: Probability 0.919\n",
      "Document 114 is assigned to topics:\n",
      "  Topic 4: Probability 0.851\n",
      "Document 115 is assigned to topics:\n",
      "  Topic 7: Probability 0.416\n",
      "  Topic 17: Probability 0.298\n",
      "  Topic 19: Probability 0.233\n",
      "Document 116 is assigned to topics:\n",
      "  Topic 15: Probability 0.856\n",
      "Document 117 is assigned to topics:\n",
      "  Topic 12: Probability 0.761\n",
      "  Topic 17: Probability 0.076\n",
      "  Topic 19: Probability 0.070\n",
      "Document 118 is assigned to topics:\n",
      "  Topic 19: Probability 0.358\n",
      "  Topic 12: Probability 0.328\n",
      "  Topic 17: Probability 0.261\n",
      "Document 119 is assigned to topics:\n",
      "  Topic 19: Probability 0.711\n",
      "  Topic 12: Probability 0.203\n",
      "Document 120 is assigned to topics:\n",
      "  Topic 19: Probability 0.930\n",
      "Document 121 is assigned to topics:\n",
      "  Topic 12: Probability 0.926\n",
      "Document 122 is assigned to topics:\n",
      "  Topic 7: Probability 0.849\n",
      "  Topic 18: Probability 0.071\n",
      "Document 123 is assigned to topics:\n",
      "  Topic 11: Probability 0.898\n",
      "Document 124 is assigned to topics:\n",
      "  Topic 12: Probability 0.913\n",
      "Document 125 is assigned to topics:\n",
      "  Topic 4: Probability 0.909\n",
      "Document 126 is assigned to topics:\n",
      "  Topic 7: Probability 0.873\n",
      "Document 127 is assigned to topics:\n",
      "  Topic 3: Probability 0.895\n",
      "Document 128 is assigned to topics:\n",
      "  Topic 15: Probability 0.926\n",
      "Document 129 is assigned to topics:\n",
      "  Topic 19: Probability 0.921\n",
      "Document 130 is assigned to topics:\n",
      "  Topic 0: Probability 0.872\n",
      "Document 131 is assigned to topics:\n",
      "  Topic 2: Probability 0.493\n",
      "  Topic 8: Probability 0.315\n",
      "  Topic 0: Probability 0.011\n",
      "Document 132 is assigned to topics:\n",
      "  Topic 11: Probability 0.622\n",
      "  Topic 12: Probability 0.260\n",
      "Document 133 is assigned to topics:\n",
      "  Topic 18: Probability 0.689\n",
      "  Topic 19: Probability 0.145\n",
      "Document 134 is assigned to topics:\n",
      "  Topic 12: Probability 0.919\n",
      "Document 135 is assigned to topics:\n",
      "  Topic 11: Probability 0.927\n",
      "Document 136 is assigned to topics:\n",
      "  Topic 3: Probability 0.539\n",
      "  Topic 12: Probability 0.313\n",
      "  Topic 1: Probability 0.037\n",
      "Document 137 is assigned to topics:\n",
      "  Topic 2: Probability 0.634\n",
      "  Topic 17: Probability 0.215\n",
      "  Topic 19: Probability 0.096\n",
      "Document 138 is assigned to topics:\n",
      "  Topic 6: Probability 0.499\n",
      "  Topic 11: Probability 0.326\n",
      "  Topic 18: Probability 0.053\n",
      "Document 139 is assigned to topics:\n",
      "  Topic 19: Probability 0.929\n",
      "Document 140 is assigned to topics:\n",
      "  Topic 6: Probability 0.542\n",
      "  Topic 19: Probability 0.211\n",
      "  Topic 15: Probability 0.014\n",
      "Document 141 is assigned to topics:\n",
      "  Topic 15: Probability 0.747\n",
      "  Topic 1: Probability 0.185\n",
      "Document 142 is assigned to topics:\n",
      "  Topic 15: Probability 0.925\n",
      "Document 143 is assigned to topics:\n",
      "  Topic 7: Probability 0.959\n",
      "Document 144 is assigned to topics:\n",
      "  Topic 2: Probability 0.936\n",
      "Document 145 is assigned to topics:\n",
      "  Topic 12: Probability 0.922\n",
      "Document 146 is assigned to topics:\n",
      "  Topic 14: Probability 0.887\n",
      "Document 147 is assigned to topics:\n",
      "  Topic 19: Probability 0.923\n",
      "Document 148 is assigned to topics:\n",
      "  Topic 15: Probability 0.899\n",
      "Document 149 is assigned to topics:\n",
      "  Topic 12: Probability 0.530\n",
      "  Topic 1: Probability 0.232\n",
      "  Topic 0: Probability 0.013\n",
      "Document 150 is assigned to topics:\n",
      "  Topic 14: Probability 0.941\n"
     ]
    }
   ],
   "source": [
    "doc_topic_distribution = [lda_model.get_document_topics(doc_tfidf) for doc_tfidf in corpus_tfidf]\n",
    "\n",
    "# Map documents to topics\n",
    "document_topics = []\n",
    "for doc_topics in doc_topic_distribution:\n",
    "    # Sort by probability \n",
    "    doc_topics.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    #taking top three topics\n",
    "    top_three_topics = doc_topics[:3]\n",
    "    document_topics.append(top_three_topics)\n",
    "\n",
    "\n",
    "for i, topics in enumerate(document_topics):\n",
    "    print(f\"Document {i} is assigned to topics:\")\n",
    "    for topic, prob in topics:\n",
    "        print(f\"  Topic {topic}: Probability {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4180919a-af61-4cfd-9b01-c4423c83178e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['norm', 'kind', 'hiding', 'massive', 'helmet', 'great', 'see', 'first', 'first', 'tell', 'guy', 'make', 'lot', 'costume', 'end', 'film', 'go', 'consistently', 'busy', 'amazing', 'project', 'work', 'sure', 'movie', 'well', 'receive', 'image', 'look', 'amazing', 'think', 'first', 'trailer', 'come', 'people', 'stun', 'silhouette', 'helmet', 'helmet', 'big', 'antler', 'unique', 'distinct', 'realize', 'guy', 'make', 'physical', 'helmet', 'wear', 'think', 'want', 'moment', 'wear', 'even', 'feeling', 'great', 'challenge', 'pretty', 'big', 'challenge', 'get', 'weight', 'right', 'balance', 'head', 'get', 'secure', 'actually', 'come', 'far', 'forehead', 'think', 'nail', 'first', 'see', 'walk', 'studio', 'pretty', 'impressive', 'let', 'talk', 'fabrication', 'bunch', 'way', 'make', 'assume', 'original', 'thing', 'model', 'correct', 'hand', 'sculpt', 'print', 'pretty', 'much', 'model', 'computer', 'scan', 'datum', 'make', 'skull', 'cap', 'fit', 'really', 'well', 'start', 'model', 'heavily', 'involved', 'minutia', 'great', 'really', 'top', 'happy', 'start', 'print', 'piece', 'interesting', 'composite', 'call', 'also', 'carbon', 'fiber', 'field', 'light', 'structurally', 'pretty', 'sound', 'many', 'piece', 'almost', 'antler', 'separate', 'cluster', 'middle', 'together', 'extra', 'one', 'add', 'large', 'one', 'separate', 'want', 'also', 'modular', 'remove', 'wear', 'partial', 'remove', 'large', 'one', 'cg', 'tailored', 'helmet', 'kind', 'come', 'balance', 'mention', 'much', 'whole', 'thing', 'weigh', 'know', 'measure', 'think', 'neighborhood', 'pound', 'light', 'light', 'somewhere', 'affix', 'head', 'wear', 'skullcap', 'underneath', 'exactly', 'attach', 'multiple', 'point', 'magnetic', 'closure', 'see', 'stuff', 'ugly', 'business', 'underneath', 'physical', 'fabrication', 'assume', 'rod', 'hold', 'printing', 'finish', 'come', 'different', 'finish', 'look', 'almost', 'metal', 'also', 'maybe', 'jade', 'get', 'finish', 'particular', 'process', 'rough', 'far', 'output', 'look', 'sand', 'casting', 'shop', 'top', 'game', 'finish', 'degree', 'beautiful', 'painter', 'super', 'talented', 'guy', 'seem', 'cholo', 'car', 'beautiful', 'mottling', 'mirror', 'chrome', 'mottle', 'multiple', 'layer', 'candy', 'green', 'mottle', 'chrome', 'mottle', 'labor', 'intensive', 'lot', 'layer', 'make', 'look', 'worldly', 'material', 'layer', 'least', 'cut', 'polished', 'know', 'beautiful', 'print', 'print', 'little', 'large', 'sand', 'rough', 'surface', 'really', 'add', 'much', 'rough', 'get', 'get', 'unbelievable', 'much', 'new', 'technology', 'else', 'work', 'talk', 'work', 'work', 'cool', 'villain', 'like', 'villain', 'talk', 'much', 'pretty', 'cool', 'pretty', 'solid', 'wait', 'people', 'start', 'see', 'imagery', 'come', 'course', 'imagery', 'see', 'person', 'great', 'see', 'show', 'get', 'get', 'close', 'real', 'helmet', 'beautiful', 'prop', 'thank', 'much', 'great', 'see', 'thank']\n"
     ]
    }
   ],
   "source": [
    "print(documents[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96320092-5b9c-4e7d-8be6-769767b0dc0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
